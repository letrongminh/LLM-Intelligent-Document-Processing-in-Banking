{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import scipy.stats as stats\n",
    "# from statsmodels.formula.api import ols\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# from tabulate import tabulate\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = \"../backend/.env\"\n",
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Markdown Content Length: 31645\n",
      "Markdown saved to: ../data/loan-prediction-eda.md\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "def write_to_markdown(markdown_content, markdown_file):\n",
    "    \"\"\"\n",
    "    Writes the markdown content to a markdown file.\n",
    "    \"\"\"\n",
    "    with open(markdown_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(markdown_content)\n",
    "    return markdown_file\n",
    "\n",
    "def extract_markdown_from_notebook(notebook_path):\n",
    "    \"\"\"\n",
    "    Extracts markdown cells from a Jupyter notebook.\n",
    "    \"\"\"\n",
    "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    \n",
    "    markdown_cells = [\n",
    "        cell[\"source\"] for cell in notebook[\"cells\"] if cell[\"cell_type\"] == \"markdown\"\n",
    "    ]\n",
    "    return \"\\n\\n\".join(markdown_cells)\n",
    "\n",
    "def notebook_to_markdown(notebook_path, markdown_path):\n",
    "    \"\"\"\n",
    "    Converts a Jupyter notebook to a markdown file by extracting markdown cells.\n",
    "    \"\"\"\n",
    "    markdown_content = extract_markdown_from_notebook(notebook_path)\n",
    "    return write_to_markdown(markdown_content, markdown_path)\n",
    "\n",
    "# Example usage\n",
    "notebook_path = \"../data/loan-prediction-eda.ipynb\"  # Replace with the correct path\n",
    "markdown_path = \"../data/loan-prediction-eda.md\"  # Replace with desired output path\n",
    "\n",
    "try:\n",
    "    extracted_markdown = extract_markdown_from_notebook(notebook_path)\n",
    "    print(\"Extracted Markdown Content Length:\", len(extracted_markdown))\n",
    "    saved_path = notebook_to_markdown(notebook_path=notebook_path, markdown_path=markdown_path)\n",
    "    print(f\"Markdown saved to: {saved_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Notebook file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Notebook Content:\n",
      "#### What is loan?\n",
      "<br>\n",
      "\"In finance, a loan is the transfer of money by one party to another with an agreement to pay it back. The recipient, or borrower, incurs a debt and is usually required to pay interest for the use of the money.\" \n",
      "<br>\n",
      "\n",
      "Resource by wikipedia: https://en.wikipedia.org/wiki/Loan\n",
      "\n",
      "#### How the loan process works?\n",
      "<br>\n",
      "According to the article in investopedia.com: \"When someone needs money, they apply for a loan from a bank, corporation, government, or other entity. The borrow\n"
     ]
    }
   ],
   "source": [
    "# Example: Extract content from a notebook\n",
    "notebook_path = \"../data/loan-prediction-eda.ipynb\"\n",
    "notebook_content = extract_markdown_from_notebook(notebook_path)\n",
    "print(\"Extracted Notebook Content:\")\n",
    "print(notebook_content[:500])  # Display first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#### What is loan?\\n<br>\\n\"In finance, a loan is the transfer of money by one party to another with an agreement to pay it back. The recipient, or borrower, incurs a debt and is usually required to pay interest for the use of the money.\" \\n<br>\\n\\nResource by wikipedia: https://en.wikipedia.org/wiki/Loan\\n#### How the loan process works?\\n<br>\\nAccording to the article in investopedia.com: \"When someone needs money, they apply for a loan from a bank, corporation, government, or other entity. The borrower may be required to provide specific details such as the reason for the loan, their financial history, Social Security number (SSN), and other information. The lender reviews this information as well as a person\\'s debt-to-income (DTI) ratio to determine if the loan can be paid back.\\n<br>\\nBased on the applicant\\'s creditworthiness, the lender either denies or approves the application. The lender must provide a reason should the loan application be denied. If the application is approved, both parties sign a contract that outlines the details of the agreement. The lender advances the proceeds of the loan, after which the borrower must repay the amount including any additional charges, such as interest.\\n<br>\\nThe terms of a loan are agreed to by each party before any money or property changes hands or is disbursed. If the lender requires collateral, the lender outlines this in the loan documents. Most loans also have provisions regarding the maximum amount of interest, in addition to other covenants, such as the length of time before repayment is required.\"\\n<br>\\n\\nResource: https://www.investopedia.com/terms/l/loan.asp\\nToday, we are going to discover the world of loan!\\n## Plan\\nWe are going to analyze a loan approval dataset that is created by Kai on Kaggle.com\\n\\n#### The Goal: \\nTo discover insights about how the loan has been approved.\\n\\n#### The Proposal: \\nUse data cleaning, EDA, data visualization and statistics techniques to analyze the loan approval insights, and build a machine learning model that practices my machine learning skills.\\n\\n#### About Dataset\\nThe loan approval dataset is a collection of financial records and associated information used to determine the eligibility of individuals or organizations for obtaining loans from a lending institution. It includes various factors such as cibil score, income, employment status, loan term, loan amount, assets value, and loan status. This dataset is commonly used in machine learning and data analysis to develop models and algorithms that predict the likelihood of loan approval based on the given features.\\n<br>\\n\\nWe can also find the dataset over kaggle.com: https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset?sort=published\\n#### About columns (Information provided by the owner)\\n\\n* `loan_id`\\n* `no_of_dependents`: Number of Dependents of the Applicant\\n* `education`: Education of the Applicant (Graduate/Not Graduate)\\n* `self_employed`: Employment Status of the Applicant\\n* `income_annum`: Annual Income of the Applicant\\t\\n* `loan_amount`: Loan Amount\\n* `loan_term`: Loan Term in Years\\n* `cibil_score`: Credit Score\\t \\n* `residential_assets_value`\\t \\n* `commercial_assets_value`\\t \\n* `luxury_assets_value`\\t \\n* `bank_asset_value`\\t \\n* `loan_status`: Loan Approval Status (Approved/Rejected)\\n\\nWe already got a basic picture of this dataset, let\\'s run some data cleaning scans as usual.\\nBy the data cleaning scans, we have confirmed:\\n1. There is no null value and duplicated value in this dataset.\\n1. `no_of_dependents`, `education`, `self_employed` and `loan_status` are categorical columns.\\n1. There are a total 4269 rows in this dataset, with 13 columns (features).\\n1. There are 2656 data with an approved `loan_status`, which is about 62.2% compared to the \"rejected\" group. The dataset is slightly imbalanced but it is acceptable and we don\\'t need to rebalance it.\\n1. Other columns are numerical.\\nThis dataset is very clean and we don\\'t need to take any extra data cleaning steps at this moment.\\nWait, not yet, I notice a small detail over the columns\\' names, take a look:\\nExcept `loan_id`, all the column names contain a space in front of the text, we need to trim them up to avoid future confusions.\\nNow the columns\\' names are good.\\n## Analyze\\nLet\\'s have an overview of each feature in this dataset.\\nAs we can see, there are some variables having positive correlation with other variables, for example `loan_amount` & `income_annum`, `luxury_assets_value` & `bank_asset_value`, `income_annum` & `luxury_assets_value`. Let\\'s start from here.\\n#### 1. Loan status and loan amount\\nLet\\'s firstly take a look at the distribution of loan amounts.\\nIt is interesting to see both approved and rejected loans have the same trends based on the different loan amounts, we haven\\'t seen a strong relationship between the loan status and the loan amount in the histogram, but how about other features? For example, annual income.\\nThere are some interesting insights we have seen over the above figure:\\n1. When annual income increases, the loan amount tends to increase. But the annual income doesn\\'t show an obvious trend with the loan status.\\n1. In this dataset, the applicants who have the lower annual income have a narrow range in loan amounts. Vise Versa, the applicants who have the higher lower annual income have a wider range in the loan amounts. Besides that the lenders will only accept the loan amount that aligns with the annual income in order to insure the applicants have ability to pay the loan back, it is easy to imagine the applicants who have higher annual income have more flexibility on the amount of the loan, whatever for themselves or for the lenders.\\n1. In this dataset, the applicants who has highest annual income has been approved when they apply for the highest loan amount (see the upper right corner of the figure), but at the same time, the applicants who have the highest annual income have chances of being rejected when they apply lower loan amount (see the middle right of the figure/lower right of the triangle). It can be caused by different lenders and other conditions of the applicants. Let\\'s use some code to take a closer look at it.\\nIn the previous section, we have noticed that applicants who have the highest annual income tried to apply for a lower loan amount, but were rejected by the lenders. By checking up the subset (showing above), we confirm that the loan amount is not the main reason that causes their applications to be rejected. Taking a closer look at this subset, I see their credit scores (`cibil_score`) are considered as \"Poor\".\\n<br>\\nAccording to Equifax, the standard of the credit scores are: 300-579: Poor. 580-669: Fair. 670-739: Good. 740-799: Very good.\\n\\nResource: https://www.equifax.com/personal/education/credit/score/what-is-a-credit-score/#:~:text=300%2D579%3A%20Poor,740%2D799%3A%20Very%20good\\n\\nLet\\'s take a look at the credit score!\\n#### 2. Credit Score\\nI am amazed to see the credit scores around 540 - 550 separate the loan status into two parts in a very clear way. In the other words, the loan status is highly related to the credit score.\\n<br>\\nIt is also interesting to see the credit score that separates the loan status is not 579 which is the highest score of the \"poor\" credit score. In the other words, the poor credit scores which are above 540 - 550 still have a good chance of being approved by loan lenders. This could be attributed to lenders\\' flexibility or specific factors that impact approval decisions.\\nHowever, we also notice a puzzling trend: some of the applicants with high credit scores (above 740) were still rejected. Few of them have only applied for a small amount of the loan which is below the median amount. What is the reason?\\nBy creating the subset, we have more than 5 applicants with more than 740 credit scores, their applications were rejected.\\n<br>\\nIn order to discover more reasons, there are 4 features that take my attention, they are `residential_assets_value`, `commercial_assets_value`, `luxury_assets_value`, `bank_asset_value`. Since we are not able to find the description of these 4 variables from the dataset, let\\'s do some research and see what they are.\\n#### 3. residential_assets_value, commercial_assets_value, luxury_assets_value, bank_asset_value\\nAccording to investopedia.com: \"Asset valuation is the process of determining the fair market value of an asset.\"\\n<br>\\nResource: https://www.investopedia.com/terms/a/assetvaluation.asp#:~:text=Key%20Takeaways-,Asset%20valuation%20is%20the%20process%20of%20determining%20the%20fair%20market,less%20intangible%20assets%20and%20liabilities.\\n\\nAs we are not able to find the clear definitions of those asset values on Google, but ChatGPT helped us out:\\n<br>\\n\"Based on the description you\\'ve provided, I can offer some general interpretations for the variables you mentioned:\\n\\n* Residential Assets Value: This is likely a measure of the total value of residential properties or real estate assets owned by the individuals or organizations in the dataset.\\n\\n* Commercial Assets Value: This could represent the total value of commercial properties or business-related assets owned by the individuals or organizations in the dataset. Commercial properties might include office buildings, retail spaces, warehouses, and similar assets.\\n\\n* Luxury Assets Value: This might refer to the total value of high-end or luxury items owned by the individuals or organizations. These could include items such as luxury vehicles, valuable artwork, jewelry, and other premium possessions.\\n\\n* Bank Asset Value: This is possibly the total value of assets held by the bank or lending institution itself. It might include cash reserves, investments, and other financial assets.\"\\nThe same as the loan amount, we are not able to see the clear trends between those asset values and the loan status, we have to do something else and see what those asset values are.\\nLet\\'s create a subset and get to know the correlation scores between those 4 asset values and other variables.\\nNow we can see some insights through the heatmap:\\n1. All the asset values have moderate to strong positive linear relationships with the annual income. As the applicants who have more annual income tend to have more flexibility on purchasing the properties with higher asset values especially the luxury assets value.\\n1. By checking the correlation scores, I have a question in my mind: why do residential assets value and commercial assets value have less positive linear relationships with the annual income, compared to other assets value? As a non-finance person, I saw these trends but was not able to find out why until I was helped by ChatGPT, what ChatGPT says:\\n* The differences in correlation strengths might be due to various factors in the data and the context of the variables:\\n    1. Nature of Assets:\\n        * Luxury assets and bank assets might have a stronger connection to an individual\\'s income. People with higher incomes might be more likely to have luxury assets or maintain bank assets.\\n        * Residential and commercial assets might be influenced by other factors such as location, real estate market trends, and investment strategies, which could result in a slightly weaker correlation with income.\\n    1. Economic Status:\\n        * People with higher incomes could afford luxury items and have larger bank assets, resulting in a tighter correlation.\\n        * Residential and commercial assets might be affected by broader economic trends and market conditions, leading to a less direct correlation with individual income.\\n    1. Diverse Income Sources:\\n\\n        * Some individuals might have diverse income sources beyond their primary job, impacting the relationship between assets and annual income.\\n    1. Data Variability:\\n\\n        * Natural variability in data could contribute to variations in correlation strength. A smaller dataset might result in less precise estimates of correlation.\\n    1. Outliers and Extreme Values:\\n\\n        * Extreme values or outliers in the data can influence correlation values. If a few individuals with extremely high income also have high asset values, it could strengthen the correlation.\\n\\n3. How can an asset value equals 0? It\\'s possible that the asset values correspond to different types of properties or assets. For instance, the residential asset might refer to a property type that has a value of 0, while the commercial asset could pertain to a different type of property, such as a commercial building.\\nLet\\'s take a more look about the asset values through the scatterplot before we move on. \\n\\n* We can see, both of `residential_assets_value`, `commercial_assets_value` create right triangles on the scatter plots, cutting the plot into two sections. This indicates that the relationship between annual income and these asset values might have some sort of threshold effect. For example, once the asset value reaches a certain point, the annual income tends to increase significantly.\\n\\n\\n* The obtuse triangles you\\'re seeing for `luxury_assets_value`, `bank_asset_value suggest` have a different type of relationship. The smaller triangle for luxury assets value compared to bank asset value might indicate that while there\\'s a strong correlation between luxury assets value and annual income, there\\'s a higher variability in annual income for any given luxury assets value. In other words, luxury assets might be a good indicator of higher annual income, but there are exceptions.\\n#### 4. Loan Term\\nOn the correlation heatmap, we may already see there is no strong to moderate linear relationship between `loan_term` and other variables. But we can still discover more, it is the funnest part of a data analysis project.\\n1. The total observations for each of the loan terms are very even, we appreciate the owner of this dataset, so we can analyze the data much easier without worrying about the balancing.\\n1. The shortest loan term in this dataset, which is 2 years, gets the one of the most chances for being approved by the lenders, compared to other loan terms, except the 4 years loan term which gets the most chances for being approved.\\n1. When the loan term is more than 4 years, the chance of being rejected have significantly increased, vise versa, the chance of being approved have dropped. Until the loan term equals to 8, both chances of being approved and rejected are tended to be normal. When the loan term reaches the 10 years, it is a loan term whose chance of being approved and rejected are approximately the same. After 10 years, the trend is becoming more consistent and the chance of being approved is slightly higher than the chance of being rejected.\\n1. In the group of 2 years of loan term, the applicants who apply for loans for more than \\\\\\\\$30,000,000 have all been approved. Similar situation as the applicants in the group of the 4 years of loan term, who apply for more than \\\\\\\\$30,000,000 loan amount. Compared to the higher loan amount, the lower loan amount has more chance of being rejected, especially when the applicants want the 4 years of loan term. In my experience, short term loans with lower loan amounts should be even easier to be paid back, compared to larger amounts. What are the reasons that cause those applications to be rejected? Let\\'s use a subset to find out the reason.\\nThe low credit scores should be one of the most important reasons that the applications have been rejected, another reason can be the annual income since most of the applicants in this subset (group) have the annual income lower than the median (5100000). Other than those 2 reasons, I guess the number of the dependents can also be a reason for rejection (in this subset, the number of dependents are all more than 2), especially when the applicants have lower annual income. \\nLet\\'s discover the insights of number of dependents!\\n#### 5. Number of dependents\\nLet\\'s firstly see the distribution of this variable.\\nIt is surprising to see the `no_of_dependents` can be discretized so well, it is going to be a big benefit of our analyzing process later.\\n<br>\\nLet\\'s take a look at if the number of dependents have any relationship with the loan status by using the crosstab function.\\nBy checking the line chart above which presents the percentage of the applicants being approved or rejected by the number of dependents, we see 2 percentage lines are very evenly, even though we can see a wave over the \"approved\" line, it might be because of the total number of the applicants. So I have to reject my guess and conclude that we are not able to see an obvious trend between `no_of_dependents` and `loan_status`.\\n#### 6. Education\\nThe counts based on different education status are approximately the same. \\n<br>\\nIn addition, on the description page of this dataset, \"`education`: Education of the Applicant (Graduate/Not Graduate)\". We have to put a question mark here, if the education means high-school education or college education?\\n<br>\\nThen, we are curious about whether education affects any of the variables here.\\nBy creating a sub-table, we can say there are no significant differences between the `education` and other variables in this dataset.\\n#### 7. Self Employed\\nSame thing as the `self_employed` -  there are no significant differences between the `self_employed` and other variables in this dataset.\\nEven though we didn\\'t find the strong relationship between `no_of_dependent`, `education`, `self_employed` and other variables in this dataset, we are still willing to discover more by using different techniques.\\n## Statistical Tests\\nIn the previous sections, we have concluded `no_of_dependents`, `education` and `self_employed` don\\'t have a significant relationship with `loan_status` or other variables. In order to confirm it, let\\'s run some statistical tests and see if that is true.\\n### Chi-square Tests\\nThe null hypothesis for the Chi-Square test is that the two variables are independent, meaning that there\\'s no relationship between them. If the p-value is below a chosen significance level (0.05), we would reject the null hypothesis, indicating that there is a significant association between two variables.\\n1. `no_of_dependents` and `loan_status`\\nThe p-value is 0.78, it indicates that there\\'s no significant evidence to reject the null hypothesis. In other words, based on the Chi-Square test, there\\'s no significant association between the `no_of_dependents` variable and the `loan_status` variable in the dataset.The data does not provide enough evidence to conclude that there is a meaningful relationship between the number of dependents and the loan approval status. The p-value of 0.78 suggests that any observed differences in the distribution of `no_of_dependents` between the two groups (Approved and Rejected) could very well have occurred by chance.\\n2. `education` and `loan_status`\\nThe p-value is 0.77, it indicates that there\\'s no significant evidence to reject the null hypothesis. In other words, based on the Chi-Square test, there\\'s no significant association between the `education` variable and the `loan_status` variable in the dataset.The data does not provide enough evidence to conclude that there is a meaningful relationship between education and the loan approval status. The p-value of 0.77 suggests that any observed differences in the distribution of `education` between the two groups (Approved and Rejected) could very well have occurred by chance.\\n3. `self_employed` and `loan_status`\\nThe p-value is 1, it indicates that there\\'s no significant evidence to reject the null hypothesis. In other words, based on the Chi-Square test, there\\'s no significant association between the `self_employed` variable and the `loan_status` variable in the dataset.The data does not provide enough evidence to conclude that there is a meaningful relationship between being self employed and the loan approval status. The p-value of 1 suggests that any observed differences in the distribution of `self_employed` between the two groups (Approved and Rejected) could very well have occurred by chance.\\n### ANOVA\\nAfter confirming `no_of_dependents`, `education` and `self_employed` don\\'t have a significant relationship with `loan_status`, we want to do one more test using ANOVA technique.\\nWe\\'d like to perform a ANOVA test by using `loan_amount` (numerical) as dependent variable, and independent variables: `no_of_dependents` (categorical), `education`(categorical) and `self_employed`(categorical). We will see how the categorical variables `no_of_dependents`, `education`, and `self_employed` might collectively impact the `loan_amount` (numerical) in an ANOVA framework. This approach can help us understand whether there are statistically significant differences in `loan_amount` based on different combinations of these categorical factors.\\nLet\\'s firstly check the assumptions:\\n1. Linearity\\nSince `no_of_dependents`, `education`, and `self_employed` are all categorical variables, checking for linearity of the dependent variable (`loan_amount` in this case) is not as crucial as it would be when we\\'re dealing with continuous independent variables. The main focus is on how the different categories of the categorical variables relate to variations in the dependent variable.\\n2. Normality \\nAs we see on the left plot, the distribution of the residuals is approximately a right skewed histogram. In the Q-Q plot on the right, there is a straight diagonal line going from the bottom left to the upper right of the Q-Q plot, and the blue markers in the Q-Q plot are relatively close to the red diagonal line and not deviating significantly, it suggests that the data is approximately normally distributed. \\n3. Independent observations\\nCombining our previous conclusions and the correlation score in the previous section, we assume all independent variables here (`no_of_dependents`, `education`, and `self_employed`) are independent from one another.\\n4. Homoscedasticity\\nThe data points seem to be scattered randomly across the line where residuals equal 0, the assumption is likely met.\\nNow we can run the ANVOA test!\\n* `C(no_of_dependents)`:\\n\\nThe p-value (0.645062) is greater than the common significance level of 0.05.\\nThis suggests that there is no significant evidence to reject the null hypothesis, indicating that the number of dependents does not have a significant relationship with the loan amount.\\n\\n* `C(education)`:\\n\\nThe p-value (0.474059) is also greater than 0.05.\\nSimilar to the previous result, there\\'s no significant evidence to suggest that education level has a significant relationship with the loan amount.\\n\\n* `C(self_employed)`:\\n\\nThe p-value (0.948694) is way above 0.05.\\nOnce again, there\\'s no significant evidence to indicate that being self-employed has a significant relationship with the loan amount.\\n<br>\\n* In all three cases, the p-values are above the common threshold of 0.05, suggesting that these categorical variables are not significantly associated with the loan amount based on the ANOVA tests.\\n## Construct\\nWe are now proceeding to the construct stage, which is the step for building a machine learning model. Before we start, we need to do some extra steps to make the variables ready for the following steps.\\nLet\\'s check the correlation score again!\\n1. Over the heatmap above, we can now see the variable that affects the `loan_status` the most is `cibil_score`.\\n1. We also proved there is no variable that has a linear relationship with the `education` and `self_employed`.\\n1. As we already mentioned over previous sections, there are strong linear relationship between `income_annum` & `loan_amount`, `luxury_assets_value` & `income_annum`, `bank_asset_value` & `income_annum` and `luxury_assets_value` & `bank_asset_value`\\nWe perfectly separate the train, validation and test sets with 60:20:20 ratio in the overall dataset.\\nWe firstly want to try a simple one, Logistic Regression.\\nThe logistic regression model was trained on the standardized features of the training data. The coefficients represent the change in the log-odds of the target variable for a one-unit increase in each standardized feature. The intercept term provides the baseline log-odds when all standardized features are zero. Positive coefficients indicate a positive association with the target variable, while negative coefficients indicate a negative association. The magnitude of the coefficients reflects the strength of the relationship between each feature and the log-odds of the outcome.\\nThe Logistic Regression model has already performed well, but we want to try a Random Forest model as well.\\nThe Random Forest model performed better performance than the Logistic Regression Model, we are going to use this one!\\nThe scores we\\'ve obtained on the test set are remarkably similar to the validation set scores, which is a positive indication that model\\'s performance generalizes well to new and unseen data. Here\\'s what the scores mean:\\n* Accuracy: Both on the validation set and the test set, we have an accuracy of around 0.972 to 0.973. This suggests that our model is correctly classifying around 97.2% to 97.3% of instances in both datasets.\\n\\n* Precision: Precision measures how many of the predicted positive instances are actually positive. With a precision of around 0.978 in both sets, it means that about 97.8% of the instances predicted as positive by our model are truly positive.\\n\\n* Recall: Recall, also known as sensitivity or true positive rate, indicates how many of the actual positive instances our model is capturing. With a recall of around 0.979 in both sets, it means that our model is correctly identifying about 97.9% of the actual positive instances.\\n\\n* F1 Score: The F1 score is the harmonic mean of precision and recall and provides a balanced view of a model\\'s performance. With an F1 score of around 0.979 in both sets, it indicates that the model is achieving a balanced trade-off between precision and recall.\\n<br>\\nThe fact that these metrics are very consistent between the validation and test sets is a great sign. It suggests that the model is not overfitting to the validation set and that it\\'s likely to perform similarly well on new, unseen data.\\n<br>\\nOverall, the Random Forest model seems to have achieved a solid and robust performance on both the validation and test sets, demonstrating its capability to make accurate predictions across different datasets.\\nThe scores in the confusion matrix above mean:\\n* True negatives (upper left): The number of applications that were rejected that the model accurately predicted were rejected.\\n* False negatives (bottom left): The number of applications that were approved that the model inaccurately predicted were rejected.\\n* False positives (upper right): The number of applications that were rejected that the model inaccurately predicted were approved.\\n* True positives (bottom right): The number of applications that were approved that the model accurately predicted were approved.\\n\\nIn our analysis, we found that `cibil_score` stands out as the most important factor influencing the model\\'s predictions. One possible reason for this prominence could be that `cibil_score` has a notable relationship with the target variable `loan_status`. However, it\\'s important to note that correlation doesn\\'t imply causation. While `cibil_score` plays a crucial role in the model\\'s predictions, it\\'s important to remember that our model identifies patterns and associations in the data. The importance of a feature doesn\\'t necessarily mean it directly causes the predicted outcomes.In the context of our problem, `cibil_score` corresponds to credit scores. We should also consider that other factors, interactions between features, and potential noise in the data can influence these findings. Therefore, a holistic understanding of the model\\'s behavior requires taking these aspects into account. The insights from the feature importance analysis can guide us in making informed decisions and refining our strategies. It\\'s important to interpret these findings alongside other domain knowledge and analysis.\\n## Conclusion\\nIn conclusion, this data analysis project aimed to explore the relationships between various variables in a dataset and the loan approval status. Through a comprehensive exploratory data analysis (EDA), we examined key features such as loan amount, annual income, loan term, asset values, and demographic factors.\\n<br>\\n1. Our analysis revealed several important insights:\\n\\n    * Loan Amount and Annual Income: We found a strong positive correlation between loan amount and annual income. As expected, individuals with higher annual incomes were more likely to secure larger loan amounts. This underscores the importance of a borrower\\'s financial capacity in determining loan approval.\\n\\n    * Loan Term and Approval Rate: Shorter loan terms, particularly 2 and 4 years, exhibited higher approval rates compared to longer terms. This suggests that borrowers opting for shorter loan durations are perceived as lower risk by lenders.\\n\\n    * Asset Values and Income: Luxury asset values and bank asset values demonstrated stronger positive correlations with annual income compared to residential and commercial asset values. This suggests that higher-income individuals are more likely to possess luxury items and maintain larger bank assets.\\n\\n    * Categorical Variables: Our ANOVA tests examined the relationships between categorical variables (no_of_dependents, education, self_employed) and loan amount. While some variables showed minor effects on loan amount, none exhibited statistically significant relationships.\\n\\n    * Machine Learning model: We used Random Forest model, which has ability to handle non-linearity, interactions between variables, and potential outliers, correctly predicted approximately 97% ~ 98% loan applications. This model allowed us to identify the most influential features in predicting loan approvals and made predictive performance remain reliable across various scenarios. While the model performs well, there\\'s always room for improvement by gathering more data, refine features, and fine-tune model parameters to enhance predictive accuracy further.\\n\\n\\n\\n1. In light of these findings, it\\'s clear that loan approval decisions are driven by a combination of financial factors, including income, loan amount, and loan term. While certain categorical variables appeared to have limited impact on loan amount, the overall focus should remain on financial indicators.\\n\\n1. It\\'s important to note that our analysis is subject to certain limitations. The dataset may not capture all relevant factors affecting loan approval, and outliers or data inaccuracies could influence results. Additionally, causation cannot be established through correlation alone.\\n\\n1. Moving forward, this analysis provides valuable insights for lenders and borrowers alike. Lenders can use these findings to refine their loan approval criteria, while borrowers can better understand the factors influencing their chances of approval.\\n\\n1. Suggestion:\\n    * Further research could delve deeper into qualitative aspects such as loan purpose, borrower credit history, and economic conditions. Additionally, considering a wider range of features and employing advanced machine learning techniques could provide a more nuanced understanding of loan approval dynamics.\\n\\n\\nIn conclusion, this analysis sheds light on the intricate relationships between various factors and loan approval outcomes. The insights gained can guide data-driven decision-making in the lending industry.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebook_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text splitter to split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "document = Document(text=notebook_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20 chunks.\n",
      "Chunk 1:\n",
      "#### What is loan?\n",
      "<br>\n",
      "\"In finance, a loan is the transfer of money by one party to another with an agreement to pay it back. The recipient, or borrower, incurs a debt and is usually required to pay interest for the use of the money.\" \n",
      "<br>\n",
      "\n",
      "Resource by wikipedia: https://en.wikipedia.org/wiki/Loan\n",
      "filename: loan-prediction-eda.md\n",
      "extension: .md\n",
      "header_path: /\n",
      "\n",
      "Chunk 2:\n",
      "#### How the loan process works?\n",
      "<br>\n",
      "According to the article in investopedia.com: \"When someone needs money, they apply for a loan from a bank, corporation, government, or other entity. The borrower may be required to provide specific details such as the reason for the loan, their financial history, Social Security number (SSN), and other information. The lender reviews this information as well as a person's debt-to-income (DTI) ratio to determine if the loan can be paid back.\n",
      "<br>\n",
      "Based on the applicant's creditworthiness, the lender either denies or approves the application. The lender must provide a reason should the loan application be denied. If the application is approved, both parties sign a contract that outlines the details of the agreement. The lender advances the proceeds of the loan, after which the borrower must repay the amount including any additional charges, such as interest.\n",
      "<br>\n",
      "The terms of a loan are agreed to by each party before any money or property changes hands or is disbursed. If the lender requires collateral, the lender outlines this in the loan documents. Most loans also have provisions regarding the maximum amount of interest, in addition to other covenants, such as the length of time before repayment is required.\"\n",
      "<br>\n",
      "\n",
      "Resource: https://www.investopedia.com/terms/l/loan.asp\n",
      "\n",
      "Today, we are going to discover the world of loan!\n",
      "filename: loan-prediction-eda.md\n",
      "extension: .md\n",
      "header_path: /What is loan?/\n",
      "\n",
      "Chunk 3:\n",
      "## Plan\n",
      "\n",
      "We are going to analyze a loan approval dataset that is created by Kai on Kaggle.com\n",
      "filename: loan-prediction-eda.md\n",
      "extension: .md\n",
      "header_path: /What is loan?/\n",
      "\n",
      "Chunk 4:\n",
      "#### The Goal: \n",
      "To discover insights about how the loan has been approved.\n",
      "filename: loan-prediction-eda.md\n",
      "extension: .md\n",
      "header_path: /What is loan?/Plan/\n",
      "\n",
      "Chunk 5:\n",
      "#### The Proposal: \n",
      "Use data cleaning, EDA, data visualization and statistics techniques to analyze the loan approval insights, and build a machine learning model that practices my machine learning skills.\n",
      "filename: loan-prediction-eda.md\n",
      "extension: .md\n",
      "header_path: /What is loan?/Plan/The Goal: /\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "def chunk_markdown(markdown_path):\n",
    "    \"\"\"\n",
    "    Chunks a markdown file into smaller segments using LlamaIndex.\n",
    "    \n",
    "    Args:\n",
    "    markdown_path (str): Path to the markdown file.\n",
    "    chunk_size (int): Number of characters per chunk (approximate).\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of TextNode objects containing the chunks.\n",
    "    \"\"\"\n",
    "    # Read the markdown file\n",
    "    markdown_content = FlatReader().load_data(Path(markdown_path))\n",
    "    \n",
    "    # Use LlamaIndex's SimpleNodeParser for chunking\n",
    "    parser = MarkdownNodeParser()\n",
    "    return markdown_content, parser.get_nodes_from_documents(markdown_content)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "markdown_path = \"../data/loan-prediction-eda.md\"  # Path to the markdown file\n",
    "\n",
    "try:\n",
    "    markdown_content, md_chunk_nodes = chunk_markdown(markdown_path)\n",
    "    print(f\"Generated {len(md_chunk_nodes)} chunks.\")\n",
    "    for i, node in enumerate(md_chunk_nodes[:5]):  # Display first 5 chunks\n",
    "        print(f\"Chunk {i+1}:\")\n",
    "        print(node.get_content())\n",
    "        print(node.get_metadata_str())\n",
    "        print()\n",
    "except FileNotFoundError:\n",
    "    print(\"Markdown file not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "huggingface_embeddings = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "openai_embeddings = OpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in md_chunk_nodes:\n",
    "    node_embedding = openai_embeddings.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Nodes into a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, Index, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"llamaindex-rag-fs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions are for text-embedding-ada-002\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"euclidean\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upserted vectors: 100%|██████████| 20/20 [00:01<00:00, 16.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c3f75d5c-afd6-4420-88f9-a517a0908e2b',\n",
       " 'c194ce2b-a020-4305-bcb0-aa0beeaddf69',\n",
       " 'a07be9d1-9fe9-4aea-bfd4-fece2292e820',\n",
       " '4ed6fbfb-5426-4202-93f2-de19a03f0403',\n",
       " '5acf7f0f-4ff2-4f0e-8844-804c80dfe122',\n",
       " 'a822768b-8015-49ff-8e90-2776a385dc03',\n",
       " 'b82c42bb-a741-43c1-9ca4-de296550cd56',\n",
       " '3e263b84-f877-4aa7-9ef1-142b082aab35',\n",
       " '204c7314-7a08-4ec9-941c-c9691a23b962',\n",
       " '4769a99b-34b0-4fc1-a4b7-0d9514fa7d2c',\n",
       " '87b21693-3c21-4dbe-8c9b-87492ebd5576',\n",
       " 'd088bf2f-5a8e-4717-9118-a4dcbfee3cb9',\n",
       " '071b2d97-1ae4-4c07-b7bf-bac9f1307fd4',\n",
       " '0737165c-d54e-4e25-8893-87f271359f57',\n",
       " '89b002d5-5eac-4b46-9ded-1df48498b00a',\n",
       " '691a666f-0f0e-4d7b-a1d3-b606935da57e',\n",
       " '3767b125-f1f5-420c-9ae2-959fc8ff843f',\n",
       " '05c507c9-8d99-4931-bccd-2d5f937664d4',\n",
       " 'b8192456-021f-4d8f-b0cd-db654553b279',\n",
       " 'ff2c9346-3719-4bd6-9977-993268ba64fa']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(md_chunk_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Query from the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"how many credit records should be to avoid being rejected ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To avoid being rejected, credit records should have credit scores above 740.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(query_str)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.NodeWithScore"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.source_nodes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node id: 4769a99b-34b0-4fc1-a4b7-0d9514fa7d2c\n",
      "Node metadata: #### 2. Credit Score\n",
      "\n",
      "I am amazed to see the credit scores around 540 - 550 separate the loan status into two parts in a very clear way. In the other words, the loan status is highly related to the credit score.\n",
      "<br>\n",
      "It is also interesting to see the credit score that separates the loan status is not 579 which is the highest score of the \"poor\" credit score. In the other words, the poor credit scores which are above 540 - 550 still have a good chance of being approved by loan lenders. This could be attributed to lenders' flexibility or specific factors that impact approval decisions.\n",
      "\n",
      "However, we also notice a puzzling trend: some of the applicants with high credit scores (above 740) were still rejected. Few of them have only applied for a small amount of the loan which is below the median amount. What is the reason?\n",
      "\n",
      "By creating the subset, we have more than 5 applicants with more than 740 credit scores, their applications were rejected.\n",
      "<br>\n",
      "In order to discover more reasons, there are 4 features that take my attention, they are `residential_assets_value`, `commercial_assets_value`, `luxury_assets_value`, `bank_asset_value`. Since we are not able to find the description of these 4 variables from the dataset, let's do some research and see what they are.\n",
      "Similarity Score: 0.373927712\n",
      "---------------------\n",
      "Node id: 204c7314-7a08-4ec9-941c-c9691a23b962\n",
      "Node metadata: #### 1. Loan status and loan amount\n",
      "\n",
      "Let's firstly take a look at the distribution of loan amounts.\n",
      "\n",
      "It is interesting to see both approved and rejected loans have the same trends based on the different loan amounts, we haven't seen a strong relationship between the loan status and the loan amount in the histogram, but how about other features? For example, annual income.\n",
      "\n",
      "There are some interesting insights we have seen over the above figure:\n",
      "1. When annual income increases, the loan amount tends to increase. But the annual income doesn't show an obvious trend with the loan status.\n",
      "1. In this dataset, the applicants who have the lower annual income have a narrow range in loan amounts. Vise Versa, the applicants who have the higher lower annual income have a wider range in the loan amounts. Besides that the lenders will only accept the loan amount that aligns with the annual income in order to insure the applicants have ability to pay the loan back, it is easy to imagine the applicants who have higher annual income have more flexibility on the amount of the loan, whatever for themselves or for the lenders.\n",
      "1. In this dataset, the applicants who has highest annual income has been approved when they apply for the highest loan amount (see the upper right corner of the figure), but at the same time, the applicants who have the highest annual income have chances of being rejected when they apply lower loan amount (see the middle right of the figure/lower right of the triangle). It can be caused by different lenders and other conditions of the applicants. Let's use some code to take a closer look at it.\n",
      "\n",
      "In the previous section, we have noticed that applicants who have the highest annual income tried to apply for a lower loan amount, but were rejected by the lenders. By checking up the subset (showing above), we confirm that the loan amount is not the main reason that causes their applications to be rejected. Taking a closer look at this subset, I see their credit scores (`cibil_score`) are considered as \"Poor\".\n",
      "<br>\n",
      "According to Equifax, the standard of the credit scores are: 300-579: Poor. 580-669: Fair. 670-739: Good. 740-799: Very good.\n",
      "\n",
      "Resource: https://www.equifax.com/personal/education/credit/score/what-is-a-credit-score/#:~:text=300%2D579%3A%20Poor,740%2D799%3A%20Very%20good\n",
      "\n",
      "\n",
      "Let's take a look at the credit score!\n",
      "Similarity Score: 0.444200873\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "# Inspect the result to see which documents are retrieved\n",
    "for node_with_score in response.source_nodes:\n",
    "    node = node_with_score.node  # Access the Node object from NodeWithScore\n",
    "    print(\"Node id:\", node.id_)  # Access the similarity score\n",
    "    print(\"Node metadata:\", node.get_content())  # Access the similarity score\n",
    "    print(\"Similarity Score:\", node_with_score.score)  # Access the similarity score\n",
    "    print(\"---------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
